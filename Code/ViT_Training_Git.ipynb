{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913c5ef-02ba-487a-9014-d4403666cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building image classification using vision transformer\n",
    "## Article: Developing Building Exposure Models Using Computer Vision and Deep Learning\n",
    "#\n",
    "# Authors: Sukh Sagar Shukla, Amit Bhatiya, Dhanya J, Saman Ghaffarian, Roberto Gentile\n",
    "#\n",
    "# Description:\n",
    "# This script provides an example implementation of training a ViT model for an image classification task\n",
    "# Please refer to the article for further details.\n",
    "\n",
    "# Folder structure of the dataset:\n",
    "# ├── Class1/\n",
    "# │   ├── image1.jpg\n",
    "# │   ├── image2.jpg\n",
    "# │   ├── image3.jpg\n",
    "# │   └── ...\n",
    "# ├── Class2/\n",
    "# │   ├── image1.jpg\n",
    "# │   ├── image2.jpg\n",
    "# │   ├── image3.jpg\n",
    "# │   └── ...\n",
    "# └── ...\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "class ViTTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        num_classes,\n",
    "        batch_size=32,  # Reduced batch size for better generalization\n",
    "        learning_rate=5e-6,  # Reduced learning rate\n",
    "        train_split=0.7,  # Updated to 70%\n",
    "        val_split=0.15,   # Added validation split 15%\n",
    "        test_split=0.15,  # Added test split 15%\n",
    "        image_size=224,\n",
    "        num_workers=10,\n",
    "        checkpoint_dir='Path to save the checkpoint' # Provide the directory path to save the checkpoints\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "        self.image_size = image_size\n",
    "        self.num_workers = num_workers\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.log_dir = Path('Path to save training logs ') # Provide the directory path to save the training logs\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self._setup_data()\n",
    "        self._setup_model()\n",
    "        self._setup_training()\n",
    "\n",
    "        # Initialize lists to store training and validation metrics\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def _setup_data(self):\n",
    "        # Simplified transforms since data is pre-augmented\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Load dataset\n",
    "        full_dataset = datasets.ImageFolder(self.data_dir, transform=data_transform)\n",
    "        \n",
    "        # Calculate splits - Fixed to use proper 70:15:15 split\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(self.train_split * total_size)\n",
    "        val_size = int(self.val_split * total_size)\n",
    "        test_size = total_size - train_size - val_size  # Ensure all samples are used\n",
    "\n",
    "        print(f\"Dataset split - Total: {total_size}, Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size, test_size], generator=generator\n",
    "        )\n",
    "        \n",
    "        # Calculate class weights for balanced sampling\n",
    "        labels = [full_dataset.targets[i] for i in self.train_dataset.indices]\n",
    "        class_counts = torch.bincount(torch.tensor(labels))\n",
    "        class_weights = 1.0 / class_counts.float()\n",
    "        sample_weights = [class_weights[label] for label in labels]\n",
    "        self.sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            sample_weights, len(sample_weights), replacement=True\n",
    "        )\n",
    "        \n",
    "        # Create data loaders with balanced sampling\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True  # Drop the last incomplete batch to avoid BatchNorm issues\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def _setup_model(self):\n",
    "        # Load with latest weights and increased dropout\n",
    "        self.model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        num_features = self.model.heads.head.in_features\n",
    "        \n",
    "        # Modified head with more regularization and complexity\n",
    "        self.model.heads = nn.Sequential(\n",
    "            nn.LayerNorm(num_features),\n",
    "            nn.Linear(num_features, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.6),  # Increased dropout\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, self.num_classes)\n",
    "        )\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze more layers for fine-tuning\n",
    "        trainable_layers = [\n",
    "            'heads',\n",
    "            'blocks.11',\n",
    "            'blocks.10',\n",
    "            'blocks.9',\n",
    "            'blocks.8',\n",
    "            'blocks.7',\n",
    "            'blocks.6',\n",
    "            'blocks.5'\n",
    "        ]\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if any(layer in name for layer in trainable_layers):\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _setup_training(self):\n",
    "        # Use label smoothing and class weights\n",
    "        class_counts = torch.bincount(torch.tensor(self.train_dataset.dataset.targets))\n",
    "        class_weights = 1.0 / class_counts.float()\n",
    "        class_weights = class_weights.to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "        \n",
    "        # Separate parameter groups with different learning rates\n",
    "        encoder_params = []\n",
    "        head_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'heads' in name:\n",
    "                    head_params.append(param)\n",
    "                else:\n",
    "                    encoder_params.append(param)\n",
    "        \n",
    "        # Modified optimizer settings\n",
    "        self.optimizer = optim.AdamW([\n",
    "            {'params': encoder_params, 'lr': self.learning_rate * 0.1},\n",
    "            {'params': head_params, 'lr': self.learning_rate}\n",
    "        ], weight_decay=0.02)  # Increased weight decay\n",
    "        \n",
    "        # Cosine annealing with warm restarts\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            T_0=5,  # Restart every 5 epochs\n",
    "            T_mult=2,  # Double the restart interval after each restart\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch_idx, (images, targets) in enumerate(pbar):\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "        \n",
    "        self.scheduler.step()\n",
    "        return total_loss / len(self.train_loader), 100. * correct / total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = torch.zeros(self.num_classes, device=self.device)\n",
    "        class_total = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        for images, targets in tqdm(self.val_loader, desc='Validating'):\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(self.num_classes):\n",
    "                mask = targets == i\n",
    "                class_correct[i] += predicted[mask].eq(targets[mask]).sum().item()\n",
    "                class_total[i] += mask.sum().item()\n",
    "        \n",
    "        # Print per-class accuracy\n",
    "        for i in range(self.num_classes):\n",
    "            if class_total[i] > 0:\n",
    "                print(f'Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')\n",
    "        \n",
    "        return total_loss / len(self.val_loader), 100. * correct / total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self):\n",
    "        \"\"\"Test the model on the test set\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = torch.zeros(self.num_classes, device=self.device)\n",
    "        class_total = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        for images, targets in tqdm(self.test_loader, desc='Testing'):\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(self.num_classes):\n",
    "                mask = targets == i\n",
    "                class_correct[i] += predicted[mask].eq(targets[mask]).sum().item()\n",
    "                class_total[i] += mask.sum().item()\n",
    "        \n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Test Loss: {total_loss / len(self.test_loader):.4f}\")\n",
    "        print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n",
    "        \n",
    "        # Print per-class test accuracy\n",
    "        for i in range(self.num_classes):\n",
    "            if class_total[i] > 0:\n",
    "                print(f'Test Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')\n",
    "        \n",
    "        return total_loss / len(self.test_loader), 100. * correct / total\n",
    "\n",
    "    def train(self, num_epochs=50):  # Increased default epochs\n",
    "        print(\"Starting training...\")\n",
    "        start_time = time.time()\n",
    "        best_val_acc = 0\n",
    "        patience = 10  # Early stopping patience\n",
    "        no_improve = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            # Append metrics to lists\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                self.save_checkpoint(epoch, val_loss, val_acc, is_best=True)\n",
    "                no_improve = 0\n",
    "                print(\"New best model saved!\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "            \n",
    "            # Regular checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(epoch, val_loss, val_acc)\n",
    "        \n",
    "        # Save training and validation metrics to a JSON file\n",
    "        metrics = {\n",
    "            'train_losses': self.train_losses,\n",
    "            'train_accuracies': self.train_accuracies,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_accuracies': self.val_accuracies\n",
    "        }\n",
    "        \n",
    "        with open(self.log_dir / 'training_metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "        \n",
    "        print(f\"\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        \n",
    "        # Run test evaluation after training\n",
    "        print(\"\\nRunning final test evaluation...\")\n",
    "        test_loss, test_acc = self.test()\n",
    "        \n",
    "        return best_val_acc, test_acc\n",
    "\n",
    "    def save_checkpoint(self, epoch, val_loss, val_acc, is_best=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            torch.save(checkpoint, self.checkpoint_dir / 'best_model.pth')\n",
    "        else:\n",
    "            torch.save(checkpoint, self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec80b5-5275-433c-ba03-a84770ee8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = ViTTrainer(\n",
    "        data_dir=\"Path to your data directory\", #enter your dataset folder\n",
    "        num_classes=24,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        train_split=0.7,   # 70% for training\n",
    "        val_split=0.15,    # 15% for validation\n",
    "        test_split=0.15    # 15% for testing\n",
    "    )\n",
    "    best_val_acc, test_acc = trainer.train(num_epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
