{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b936c-970c-4e87-9922-506e799bd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building image classification using vision transformer\n",
    "## Article: Developing Building Exposure Models Using Computer Vision and Deep Learning\n",
    "#\n",
    "# Authors: Sukh Sagar Shukla, Amit Bhatiya, Dhanya J, Saman Ghaffarian, Roberto Gentile\n",
    "#\n",
    "# Description:\n",
    "# This script provides an example implementation of training a BEiT model for an image classification task\n",
    "# Please refer to the article for further details.\n",
    "\n",
    "# Folder structure of the dataset:\n",
    "# ├── Class1/\n",
    "# │   ├── image1.jpg\n",
    "# │   ├── image2.jpg\n",
    "# │   ├── image3.jpg\n",
    "# │   └── ...\n",
    "# ├── Class2/\n",
    "# │   ├── image1.jpg\n",
    "# │   ├── image2.jpg\n",
    "# │   ├── image3.jpg\n",
    "# │   └── ...\n",
    "# └── ...\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BeitForImageClassification, BeitConfig\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "class BEiTTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        num_classes,\n",
    "        batch_size=32,\n",
    "        learning_rate=5e-6,\n",
    "        train_split=0.7,\n",
    "        val_split=0.15,\n",
    "        test_split=0.15,\n",
    "        image_size=224,\n",
    "        num_workers=10,\n",
    "        checkpoint_dir='Path to save the checkpoint' # Provide the directory path to save the checkpoints\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "        self.image_size = image_size\n",
    "        self.num_workers = num_workers\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.log_dir = Path('Path to save training logs ') # Provide the directory path to save the training logs\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Lists to store metrics\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        self.train_class_accuracies = []  # Per-class accuracy for training\n",
    "        self.val_class_accuracies = []    # Per-class accuracy for validation\n",
    "        self.test_class_accuracies = []   # Per-class accuracy for testing\n",
    "\n",
    "        self._setup_data()\n",
    "        self._setup_model()\n",
    "        self._setup_training()\n",
    "\n",
    "    def _setup_data(self):\n",
    "        # BEiT specific transforms\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # BEiT normalization\n",
    "        ])\n",
    "\n",
    "        # Load dataset\n",
    "        full_dataset = datasets.ImageFolder(self.data_dir, transform=data_transform)\n",
    "        \n",
    "        # Calculate splits (70:15:15)\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(self.train_split * total_size)\n",
    "        val_size = int(self.val_split * total_size)\n",
    "        test_size = total_size - train_size - val_size  # Remaining samples go to test\n",
    "        \n",
    "        print(f\"Dataset split - Total: {total_size}, Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "        \n",
    "        # Create splits with fixed seed\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size, test_size], generator=generator\n",
    "        )\n",
    "        \n",
    "        # Calculate class weights for balanced sampling\n",
    "        labels = [full_dataset.targets[i] for i in self.train_dataset.indices]\n",
    "        class_counts = torch.bincount(torch.tensor(labels))\n",
    "        class_weights = 1.0 / class_counts.float()\n",
    "        sample_weights = [class_weights[label] for label in labels]\n",
    "        self.sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            sample_weights, len(sample_weights), replacement=True\n",
    "        )\n",
    "        \n",
    "        # Create data loaders with balanced sampling\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def _setup_model(self):\n",
    "        # Initialise BEiT model with custom configuration\n",
    "        config = BeitConfig.from_pretrained(\n",
    "            \"microsoft/beit-base-patch16-224-pt22k-ft22k\",\n",
    "            num_labels=self.num_classes,\n",
    "            id2label={str(i): str(i) for i in range(self.num_classes)},\n",
    "            label2id={str(i): i for i in range(self.num_classes)}\n",
    "        )\n",
    "        \n",
    "        self.model = BeitForImageClassification.from_pretrained(\n",
    "            \"microsoft/beit-base-patch16-224-pt22k-ft22k\",\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze specific layers for fine-tuning\n",
    "        trainable_layers = [\n",
    "            'pooler',\n",
    "            'classifier',\n",
    "            'encoder.layer.11',\n",
    "            'encoder.layer.10',\n",
    "            'encoder.layer.9',\n",
    "            'encoder.layer.8',\n",
    "            'encoder.layer.7',\n",
    "            'encoder.layer.6',\n",
    "            'encoder.layer.5'\n",
    "        ]\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if any(layer in name for layer in trainable_layers):\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _setup_training(self):\n",
    "        # Use label smoothing and class weights\n",
    "        class_counts = torch.bincount(torch.tensor(self.train_dataset.dataset.targets))\n",
    "        class_weights = 1.0 / class_counts.float()\n",
    "        class_weights = class_weights.to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "        \n",
    "        # Separate parameter groups with different learning rates\n",
    "        encoder_params = []\n",
    "        head_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(x in name for x in ['pooler', 'classifier']):\n",
    "                    head_params.append(param)\n",
    "                else:\n",
    "                    encoder_params.append(param)\n",
    "        \n",
    "        # Modified optimiser settings\n",
    "        self.optimizer = optim.AdamW([\n",
    "            {'params': encoder_params, 'lr': self.learning_rate * 0.1},\n",
    "            {'params': head_params, 'lr': self.learning_rate}\n",
    "        ], weight_decay=0.08)\n",
    "        \n",
    "        # Cosine annealing with warm restarts\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            T_0=5,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = torch.zeros(self.num_classes, device=self.device)\n",
    "        class_total = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch_idx, (images, targets) in enumerate(pbar):\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs.logits, targets)\n",
    "            \n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(self.num_classes):\n",
    "                mask = targets == i\n",
    "                class_correct[i] += predicted[mask].eq(targets[mask]).sum().item()\n",
    "                class_total[i] += mask.sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "        \n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        class_accuracies = []\n",
    "        for i in range(self.num_classes):\n",
    "            if class_total[i] > 0:\n",
    "                class_accuracies.append(100. * class_correct[i] / class_total[i])\n",
    "            else:\n",
    "                class_accuracies.append(float('nan'))  # If no samples for this class\n",
    "        \n",
    "        return total_loss / len(self.train_loader), 100. * correct / total, class_accuracies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = torch.zeros(self.num_classes, device=self.device)\n",
    "        class_total = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        for images, targets in tqdm(self.val_loader, desc='Validating'):\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs.logits, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(self.num_classes):\n",
    "                mask = targets == i\n",
    "                class_correct[i] += predicted[mask].eq(targets[mask]).sum().item()\n",
    "                class_total[i] += mask.sum().item()\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        class_accuracies = []\n",
    "        for i in range(self.num_classes):\n",
    "            if class_total[i] > 0:\n",
    "                class_accuracies.append(100. * class_correct[i] / class_total[i])\n",
    "            else:\n",
    "                class_accuracies.append(float('nan'))  # If no samples for this class\n",
    "        \n",
    "        return total_loss / len(self.val_loader), 100. * correct / total, class_accuracies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = torch.zeros(self.num_classes, device=self.device)\n",
    "        class_total = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        for images, targets in tqdm(self.test_loader, desc='Testing'):\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs.logits, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(self.num_classes):\n",
    "                mask = targets == i\n",
    "                class_correct[i] += predicted[mask].eq(targets[mask]).sum().item()\n",
    "                class_total[i] += mask.sum().item()\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        class_accuracies = []\n",
    "        for i in range(self.num_classes):\n",
    "            if class_total[i] > 0:\n",
    "                class_accuracies.append(100. * class_correct[i] / class_total[i])\n",
    "            else:\n",
    "                class_accuracies.append(float('nan'))  # If no samples for this class\n",
    "        \n",
    "        return total_loss / len(self.test_loader), 100. * correct / total, class_accuracies\n",
    "\n",
    "    def train(self, num_epochs=50):\n",
    "        print(\"Starting training...\")\n",
    "        start_time = time.time()\n",
    "        best_val_acc = 0\n",
    "        patience = 10\n",
    "        no_improve = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            train_loss, train_acc, train_class_acc = self.train_epoch()\n",
    "            val_loss, val_acc, val_class_acc = self.validate()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            self.train_class_accuracies.append(train_class_acc)\n",
    "            self.val_class_accuracies.append(val_class_acc)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                self.save_checkpoint(epoch, val_loss, val_acc, is_best=True)\n",
    "                no_improve = 0\n",
    "                print(\"New best model saved!\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "            \n",
    "            # Save logs\n",
    "            self.save_logs(epoch)\n",
    "        \n",
    "        # Final test evaluation\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        test_loss, test_acc, test_class_acc = self.test()\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        self.test_class_accuracies.append(test_class_acc)\n",
    "        \n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "        print(f\"\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"Final test accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    def save_checkpoint(self, epoch, val_loss, val_acc, is_best=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            torch.save(checkpoint, self.checkpoint_dir / 'best_model.pth')\n",
    "        else:\n",
    "            torch.save(checkpoint, self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pth')\n",
    "\n",
    "    def save_logs(self, epoch):\n",
    "        # Convert tensors to Python lists or floats\n",
    "        train_class_acc = [acc.item() if isinstance(acc, torch.Tensor) else acc for acc in self.train_class_accuracies[-1]]\n",
    "        val_class_acc = [acc.item() if isinstance(acc, torch.Tensor) else acc for acc in self.val_class_accuracies[-1]]\n",
    "        \n",
    "        log_data = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': self.train_losses[-1],\n",
    "            'train_acc': self.train_accuracies[-1],\n",
    "            'val_loss': self.val_losses[-1],\n",
    "            'val_acc': self.val_accuracies[-1],\n",
    "            'train_class_acc': train_class_acc,  # Converted to Python list\n",
    "            'val_class_acc': val_class_acc      # Converted to Python list\n",
    "        }\n",
    "        \n",
    "        # Save logs to a JSON file\n",
    "        with open(self.log_dir / f'epoch_{epoch}.json', 'w') as f:\n",
    "            json.dump(log_data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22810b-9bee-4196-b6ab-7b45a786f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = BEiTTrainer(\n",
    "        data_dir=\"Path to your data directory\", #enter your dataset folder\n",
    "        num_classes=24,\n",
    "        batch_size=64,\n",
    "        learning_rate=5e-5,\n",
    "        train_split=0.7,  # 70% for training\n",
    "        val_split=0.15,   # 15% for validation\n",
    "        test_split=0.15   # 15% for testing\n",
    "    )\n",
    "    trainer.train(num_epochs=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
