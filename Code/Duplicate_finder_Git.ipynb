{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a6f4c-676d-49c6-be8f-9b74c4c8aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set image size expected by EfficientNetB7\n",
    "IMG_SIZE = (600, 600)  # Adjust if needed\n",
    "FEATURES_PATH = \"path/to/save/features.pkl\" #Provide the path to save the feature\n",
    "\n",
    "# Load Pretrained EfficientNetB7\n",
    "def load_cnn_model():\n",
    "    base_model = EfficientNetB7(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "    return base_model\n",
    "\n",
    "# Extract Features from an Image\n",
    "def extract_features(img_path, model):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)  # EfficientNet preprocessing\n",
    "    features = model.predict(img)\n",
    "    return features.flatten()  # Convert to 1D vector\n",
    "\n",
    "# Process All Images in a Folder\n",
    "def process_images(folder_path, model):\n",
    "    features = []\n",
    "    image_paths = []\n",
    "    \n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                feat = extract_features(img_path, model)\n",
    "                features.append(feat)\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_name}: {e}\")\n",
    "    \n",
    "    return np.array(features), image_paths\n",
    "\n",
    "# Extract Features for Training Images\n",
    "def extract_train_data(folder_path):\n",
    "    model = load_cnn_model()\n",
    "    print(\"Extracting features...\")\n",
    "    features, image_paths = process_images(folder_path, model)\n",
    "\n",
    "    # Save extracted features for future use\n",
    "    with open(FEATURES_PATH, \"wb\") as f:\n",
    "        pickle.dump((features, image_paths), f)\n",
    "\n",
    "    print(\"Feature extraction complete. Saved to 'features.pkl'.\")\n",
    "    return features, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8874b-b436-4259-8485-c5c99d407ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder path containing images\n",
    "folder_path = \"path/for/folder/containing/images\"  # Replace with your actual image folder path\n",
    "\n",
    "# Extract features from images in the folder\n",
    "features, image_paths = extract_train_data(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8571f81-ceae-42b9-ab68-d699a3d35ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extracted features\n",
    "with open(\"path/for/features.pkl\", \"rb\") as f:\n",
    "    features, image_paths = pickle.load(f)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.26, min_samples=2, metric=\"cosine\") # The parameters eps=0.26 and min samples=2 work well the current building data and \n",
    "cluster_labels = dbscan.fit_predict(features)             # sample size of 200. Users are encouraged to fine-tune these parameters \n",
    "                                                          # for their data and sample size \n",
    "# Assign DBSCAN Clusters to Images\n",
    "cluster_dict = {}\n",
    "for idx, cluster_id in enumerate(cluster_labels):\n",
    "    if cluster_id not in cluster_dict:\n",
    "        cluster_dict[cluster_id] = []\n",
    "    cluster_dict[cluster_id].append(image_paths[idx])\n",
    "\n",
    "# Display Cluster Information and Images\n",
    "print(\"\\n--- Clustering Results ---\")\n",
    "for cluster_id, img_list in cluster_dict.items():\n",
    "    print(f\"\\nCluster {cluster_id}: {len(img_list)} images\")\n",
    "\n",
    "    if cluster_id == -1:\n",
    "        print(\"Noise points detected (not assigned to any cluster). Skipping visualization.\")\n",
    "        continue\n",
    "\n",
    "    # Show first 5 images in the cluster\n",
    "    fig, axes = plt.subplots(1, min(5, len(img_list)), figsize=(15, 5))\n",
    "    if len(img_list) == 1:\n",
    "        axes = [axes]  # Ensure it's iterable if only one image\n",
    "\n",
    "    for ax, img_path in zip(axes, img_list[:5]):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Cluster {cluster_id}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2390b557-1a2c-454a-92fc-243bb40877be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sorting complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# Define paths\n",
    "FEATURES_PATH = \"path/for/saved/features.pkl\"\n",
    "ORIGINAL_FOLDER = \"path to save the original image\"\n",
    "DUPLICATE_FOLDER = \"path to save the duplicate images\"\n",
    "\n",
    "# Load extracted features and image paths\n",
    "with open(FEATURES_PATH, \"rb\") as f:\n",
    "    features, image_paths = pickle.load(f)\n",
    "\n",
    "# Apply DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# dbscan = DBSCAN(eps=0.1, min_samples=2, metric=\"cosine\")\n",
    "# cluster_labels = dbscan.fit_predict(features)\n",
    "\n",
    "# Create cluster dictionary\n",
    "cluster_dict = {}\n",
    "for idx, cluster_id in enumerate(cluster_labels):\n",
    "    if cluster_id not in cluster_dict:\n",
    "        cluster_dict[cluster_id] = []\n",
    "    cluster_dict[cluster_id].append(image_paths[idx])\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(ORIGINAL_FOLDER, exist_ok=True)\n",
    "os.makedirs(DUPLICATE_FOLDER, exist_ok=True)\n",
    "\n",
    "# Move images based on clustering\n",
    "for cluster_id, img_list in cluster_dict.items():\n",
    "    if cluster_id == -1:  # Noise images\n",
    "        for img in img_list:\n",
    "            shutil.move(img, os.path.join(ORIGINAL_FOLDER, os.path.basename(img)))\n",
    "    else:  # Clustered images\n",
    "        # Move first image to 'original'\n",
    "        shutil.move(img_list[0], os.path.join(ORIGINAL_FOLDER, os.path.basename(img_list[0])))\n",
    "\n",
    "        # Move the rest to 'duplicate'\n",
    "        for img in img_list[1:]:\n",
    "            shutil.move(img, os.path.join(DUPLICATE_FOLDER, os.path.basename(img)))\n",
    "\n",
    "print(\"Image sorting complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
